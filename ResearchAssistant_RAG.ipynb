{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d9f118e",
   "metadata": {},
   "source": [
    "# AI-Driven Research Assistant Powered by Agentic RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee0de6a",
   "metadata": {},
   "source": [
    "__RAG__ (Retrieval-Augmented Generation) is a technique for enhancing teh accuracy and reliability of generative AI models with facts fetched from external sources.\n",
    "- __Retrieve__ most relevant data\n",
    "- __Augment__ query with context\n",
    "- __Generate__ response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a14eb",
   "metadata": {},
   "source": [
    "### Starting with basic RAG implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3563ed20",
   "metadata": {},
   "source": [
    "__Import required dependencies for LlamaIndex and OpenAI__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ab47d",
   "metadata": {},
   "source": [
    "`OpenAI GPT-3.5-turbo` is used by default.  \n",
    "OpenAI API key is stored in `.env` file in the project directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a0ca4f-9819-4eb9-a2a8-b11c89a0a77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f5db5df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "78caea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "53b6fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97310a7",
   "metadata": {},
   "source": [
    "__Load research paper files from local directory__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f957cf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"data\\data1\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65bb072",
   "metadata": {},
   "source": [
    "__Initialize an index and create a query engine__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "52e4123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22360601",
   "metadata": {},
   "source": [
    "The LlamaIndex default query engine combines Retriever, PostProcessing and Synthesizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c730ab7c",
   "metadata": {},
   "source": [
    "__Ask LLM questions from the local research document__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "650c84ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers can outperform RNNs in machine translation tasks due to their unique architecture based entirely on attention mechanisms. By replacing the recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention, transformers can capture long-range dependencies more effectively. This allows them to process input sequences in parallel, making them more efficient and faster to train compared to RNNs. Additionally, transformers are better at handling context information across the entire input sequence, leading to improved translation quality and performance.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Explain how transformers can outperform RNNs in machine translation tasks.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a93977a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer model architecture consists of an encoder and a decoder, both composed of a stack of identical layers. Each layer in the encoder has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Residual connections and layer normalization are applied around each sub-layer. The decoder, in addition to the two sub-layers in each layer, inserts a third sub-layer that performs multi-head attention over the encoder stack's output. The model uses residual connections and layer normalization in the decoder as well. The self-attention mechanism in the decoder is modified to prevent positions from attending to subsequent positions. This architecture allows for efficient sequence transduction based entirely on attention, without the need for recurrent layers commonly used in encoder-decoder models.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Explain the transformer model architecture.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756d82c",
   "metadata": {},
   "source": [
    "### Creating our custom RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9001a26",
   "metadata": {},
   "source": [
    "__Import required dependencies from LlamaIndex__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "49abde53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b31febf",
   "metadata": {},
   "source": [
    "__LLM and embedding model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "59f52290",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5c9c7",
   "metadata": {},
   "source": [
    "__Custom retriever__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7877f4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528b278",
   "metadata": {},
   "source": [
    "__Custom synthesizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8ff9fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a77322",
   "metadata": {},
   "source": [
    "__Custom query engine__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b6f896c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a4046",
   "metadata": {},
   "source": [
    "__Query with custom query engine__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6c7e8dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer model architecture consists of an encoder and a decoder, each composed of a stack of identical layers. The encoder stack includes self-attention layers where each position can attend to all positions in the previous layer. The decoder stack contains self-attention layers as well, allowing each position to attend to all positions up to that point. Additionally, the decoder uses encoder-decoder attention layers to enable every position in the decoder to attend over all positions in the input sequence. The model also incorporates positional encodings to provide information about the relative or absolute position of tokens in the sequence. Furthermore, the architecture includes feed-forward networks applied to each position separately and identically, along with embeddings and softmax functions for token conversion and prediction.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"Explain the transformer model architecture.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b07dd",
   "metadata": {},
   "source": [
    "__Custom prompt__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84fd676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt_str = \"\"\"\\\n",
    "Context information is below.\n",
    "--------------------------\n",
    "{context_str}\n",
    "--------------------------\n",
    "Given the context information, answer the query.\n",
    "Query: {query_str}\n",
    "Answer: \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83a58901",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_tmpl = PromptTemplate(qa_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0bb380a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context information is below.\n",
      "--------------------------\n",
      "\n",
      "--------------------------\n",
      "Given the context information, answer the query.\n",
      "Query: Explain the transformer model architecture.\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "fmt_prompt = prompt_tmpl.format(\n",
    "    context_str=\"\",\n",
    "    query_str=\"Explain the transformer model architecture.\"\n",
    ")\n",
    "print(fmt_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d056700",
   "metadata": {},
   "source": [
    "__Query with custom prompt__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7dbd4f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer model architecture consists of an encoder and a decoder, each composed of a stack of identical layers. The encoder stack includes two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The self-attention mechanism allows each position in the encoder to attend to all positions in the previous layer. The decoder stack also contains two sub-layers from the encoder, along with an additional sub-layer that performs multi-head attention over the encoder stack's output. The model utilizes residual connections and layer normalization around each sub-layer to facilitate learning. Additionally, the model incorporates positional encodings to provide information about the order of tokens in the sequence. The Transformer model is based entirely on attention mechanisms, eliminating the need for recurrence and convolutions, allowing for more parallelization and achieving state-of-the-art results in machine translation tasks.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(fmt_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d0354",
   "metadata": {},
   "source": [
    "__Combined custom prompt method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "491cfa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prompt_template(qa_prompt_str, context_str=\"\", query_str=\"\"):\n",
    "    prompt_tmpl = PromptTemplate(qa_prompt_str)\n",
    "    fmt_prompt = fmt_prompt = prompt_tmpl.format(\n",
    "        context_str=context_str,\n",
    "        query_str=query_str\n",
    "    )\n",
    "    return fmt_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c622dd",
   "metadata": {},
   "source": [
    "__Query with combined custom prompt method__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4ffaafe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper \"Attention Is All You Need\" introduces the Transformer model, which is a sequence transduction model based solely on attention mechanisms, eliminating the need for recurrent or convolutional neural networks in the encoder-decoder architecture. The Transformer model uses stacked self-attention and point-wise fully connected layers for both the encoder and decoder. It achieves state-of-the-art results in machine translation tasks, such as English-to-German and English-to-French, outperforming previous models in terms of quality and training efficiency. The model allows for more parallelization, faster training times, and improved translation quality compared to traditional models based on recurrent layers.\n"
     ]
    }
   ],
   "source": [
    "query_str = \"Summerize the 'Attention is all you need' paper.\"\n",
    "\n",
    "response = query_engine.query(custom_prompt_template(qa_prompt_str, query_str=query_str))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ad75f",
   "metadata": {},
   "source": [
    "### Advanced queries with agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7792ceca",
   "metadata": {},
   "source": [
    "__Import required dependencies from LlamaIndex__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8ad289e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d75ac147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.selectors import PydanticSingleSelector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a68f9b",
   "metadata": {},
   "source": [
    "__Documents, incides and query engines for research paper 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3d4f998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = SimpleDirectoryReader(\"data\\data1\").load_data()\n",
    "index1 = VectorStoreIndex.from_documents(document1)\n",
    "queryEngine1 = index1.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9bdedd",
   "metadata": {},
   "source": [
    "__Documents, incides and query engines for research paper 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "edcbe7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "document2 = SimpleDirectoryReader(\"data\\data2\").load_data()\n",
    "index2 = VectorStoreIndex.from_documents(document2)\n",
    "queryEngine2 = index2.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1543d7e0",
   "metadata": {},
   "source": [
    "__Query with research paper 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "aa186b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper discusses the Transformer model, which is a sequence transduction model based entirely on attention. It replaces the recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention. The paper highlights that the Transformer can be trained faster than architectures based on recurrent or convolutional layers, achieving state-of-the-art results on translation tasks. The title of the paper is \"attention is all you need.\"\n"
     ]
    }
   ],
   "source": [
    "response1 = queryEngine1.query(\"What is this paper about? And what is the title of the paper?\")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18391cad",
   "metadata": {},
   "source": [
    "__Query with research paper 2__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9ebbf22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper discusses the challenges of detecting machine-generated text and proposes a method using probability curvature for zero-shot detection. The title of the paper is \"Zero-Shot Machine-Generated Text Detection using Probability Curvature.\"\n"
     ]
    }
   ],
   "source": [
    "response2 = queryEngine2.query(\"What is this paper about? And what is the title of the paper?\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37bc713",
   "metadata": {},
   "source": [
    "__Router Query Engine__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd1f20",
   "metadata": {},
   "source": [
    "Router Query Engines can take multiple query engines and use them as tools. Each query engine has a description with the help of which the LLM will decide which tool to use to answer specific types of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "be7ed527",
   "metadata": {},
   "outputs": [],
   "source": [
    "routed_query_engine = RouterQueryEngine(\n",
    "    selector=PydanticSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        QueryEngineTool.from_defaults(\n",
    "            query_engine=queryEngine1,\n",
    "            description=(\n",
    "                \"Useful for retrieving specific context for transformer architecture which relies solely\"\n",
    "                \" on attention mechanisms and eliminates the need for recurrence in sequence transduction models\"\n",
    "            ),\n",
    "        ),\n",
    "        \n",
    "        QueryEngineTool.from_defaults(\n",
    "            query_engine=queryEngine2,\n",
    "            description=(\n",
    "                \"Useful for retrieving specific context for the challenges of detecting machine-generated text\"\n",
    "                \" and proposes a method using probability curvature for zero-shot detection\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d757ffd",
   "metadata": {},
   "source": [
    "__Query with router query engine__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "7e111a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Transformer model is specifically built for machine translation tasks and has been shown to outperform previous state-of-the-art models on tasks like English-to-German and English-to-French translation.\n"
     ]
    }
   ],
   "source": [
    "res4 = routed_query_engine.query(\"What machine learning method is specifically built and is best for machine translation tasks?\")\n",
    "print(res4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b98cba47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5da5998b-58da-4654-9841-419c4d33eb19': {'file_path': 'D:\\\\LLMProjects\\\\ResearchAssistant_RAG\\\\data\\\\data1\\\\NIPS-2017-attention-is-all-you-need-Paper.txt',\n",
       "  'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 33049,\n",
       "  'creation_date': '2024-07-10',\n",
       "  'last_modified_date': '2024-07-10'},\n",
       " '72be0f23-b858-4beb-a65a-b983c34ef80c': {'file_path': 'D:\\\\LLMProjects\\\\ResearchAssistant_RAG\\\\data\\\\data1\\\\NIPS-2017-attention-is-all-you-need-Paper.txt',\n",
       "  'file_name': 'NIPS-2017-attention-is-all-you-need-Paper.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 33049,\n",
       "  'creation_date': '2024-07-10',\n",
       "  'last_modified_date': '2024-07-10'},\n",
       " 'selector_result': MultiSelection(selections=[SingleSelection(index=0, reason='The transformer architecture, which relies solely on attention mechanisms and eliminates the need for recurrence, is specifically built for machine translation tasks.')])}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res4.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "76231e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiSelection(selections=[SingleSelection(index=0, reason='The transformer architecture, which relies solely on attention mechanisms and eliminates the need for recurrence, is specifically built for machine translation tasks.')])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res4.metadata['selector_result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc24f2f",
   "metadata": {},
   "source": [
    "__The `queryEngine1` is used to answer this query which has knowledge of the research file `NIPS-2017-attention-is-all-you-need-Paper.txt`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8f3e2117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine-generated text can be identified by comparing the log probability of a candidate passage under a particular source model with the average log probability of several perturbations of the passage under the same source model. If the perturbed passages tend to have lower average log probability than the original passage, then the candidate passage is likely to have come from the source model.\n"
     ]
    }
   ],
   "source": [
    "res5 = routed_query_engine.query(\"How can machine generated text be identified?\")\n",
    "print(res5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e9d7b1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'9a5ffd6a-f3c1-433c-b8fb-d572f0fe2c67': {'file_path': 'D:\\\\LLMProjects\\\\ResearchAssistant_RAG\\\\data\\\\data2\\\\2301.11305v2.txt',\n",
       "  'file_name': '2301.11305v2.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 62736,\n",
       "  'creation_date': '2024-07-10',\n",
       "  'last_modified_date': '2024-07-10'},\n",
       " '2efdb51e-393d-4ed5-8085-110d5557d5c5': {'file_path': 'D:\\\\LLMProjects\\\\ResearchAssistant_RAG\\\\data\\\\data2\\\\2301.11305v2.txt',\n",
       "  'file_name': '2301.11305v2.txt',\n",
       "  'file_type': 'text/plain',\n",
       "  'file_size': 62736,\n",
       "  'creation_date': '2024-07-10',\n",
       "  'last_modified_date': '2024-07-10'},\n",
       " 'selector_result': MultiSelection(selections=[SingleSelection(index=1, reason=\"The choice (2) is more relevant to the question 'How can machine generated text be identified?' as it specifically addresses the challenges of detecting machine-generated text and proposes a method using probability curvature for zero-shot detection.\")])}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res5.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "837121cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiSelection(selections=[SingleSelection(index=1, reason=\"The choice (2) is more relevant to the question 'How can machine generated text be identified?' as it specifically addresses the challenges of detecting machine-generated text and proposes a method using probability curvature for zero-shot detection.\")])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res5.metadata['selector_result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fde87a3",
   "metadata": {},
   "source": [
    "__The `queryEngine2` is used to answer this query which has knowledge of the research file `2301.11305v2.txt`.__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f16fc",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ec5ba",
   "metadata": {},
   "source": [
    "__Simple functions that an agent can use__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d7355dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a, b):\n",
    "    \"\"\"Multiple two numbers and returns the result\"\"\"\n",
    "    return a * b\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "beb33fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    \"\"\"Add two numbers and returns the result\"\"\"\n",
    "    return a + b\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588dbba0",
   "metadata": {},
   "source": [
    "__Convert previous router query engine into a tool__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cc564b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=routed_query_engine,\n",
    "    description=\"A tool that can answer questions about different research papers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa185c",
   "metadata": {},
   "source": [
    "__OpenAI Agent that can utilize these tools__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "3899ba5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OpenAIAgent.from_tools(\n",
    "    tools=[query_engine_tool, multiply_tool, add_tool],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7823da",
   "metadata": {},
   "source": [
    "__Queries that utilizes specific tools__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86a01ba",
   "metadata": {},
   "source": [
    "Queries using the router query engine tool. Each query is then further routed to use the relevent documents and query engine for with knowledge of specific research paper.  \n",
    "\n",
    "We can see the functions being called by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "cac4d95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is the secret that makes transformers the best for machine translation tasks? Explain.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\":\"The secret that makes transformers the best for machine translation tasks\"}\n",
      "Got output: The secret that makes transformers the best for machine translation tasks is their architecture based solely on attention mechanisms, which allows for drawing global dependencies between input and output without the need for recurrence or convolutions. This design enables more parallelization, faster training times, and superior translation quality compared to models that rely on recurrent or convolutional layers.\n",
      "========================\n",
      "\n",
      "The secret that makes transformers the best for machine translation tasks is their architecture based solely on attention mechanisms. This design allows transformers to draw global dependencies between input and output without the need for recurrence or convolutions. As a result, transformers enable more parallelization, faster training times, and superior translation quality compared to models that rely on recurrent or convolutional layers.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is the secret that makes transformers the best for machine translation tasks? Explain.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "ed88256a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: I think my students are submitting assignments with the help of ChatGPT. How can I detect if the work is done by using AI?\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\":\"Detecting assignments done using AI\"}\n",
      "Got output: DetectGPT is a method used for detecting machine-generated text, specifically from models like GPT-3 and Jurassic-2 Jumbo. It compares favorably with supervised models trained for machine-generated text detection, showing strong performance in detecting text generated by these AI models.\n",
      "========================\n",
      "\n",
      "To detect if the work submitted by your students is done using AI, you can use a method called DetectGPT. This method is specifically designed to detect machine-generated text, particularly from models like GPT-3 and Jurassic-2 Jumbo. DetectGPT compares favorably with supervised models trained for machine-generated text detection and shows strong performance in identifying text generated by these AI models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"I think my students are submitting assignments with the help of ChatGPT. How can I detect if the work is done by using AI?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1e26f273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: How can machine generated text be identified? Use the tools.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Identifying machine-generated text\"}\n",
      "Got output: DetectGPT is a zero-shot method for identifying machine-generated text by comparing the log probability of a candidate passage under a specific source model with the average log probability of several perturbations of the passage under the same model. If the perturbed passages consistently have lower average log probability than the original passage, it is likely that the candidate passage was generated by the source model. This method leverages the observation that machine-generated text tends to lie in regions with negative curvature in the log probability function, while human-written text does not exhibit this tendency.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Using AI tools for text identification\"}\n",
      "Got output: AI tools can be utilized for text identification, including detecting AI-generated text, analyzing convolutional traces for deepfake detection, and measuring how models mimic human falsehoods. These tools can also be used for authorship attribution in neural text generation and defending against neural fake news.\n",
      "========================\n",
      "\n",
      "Machine-generated text can be identified using the following methods:\n",
      "\n",
      "1. **DetectGPT**: DetectGPT is a zero-shot method for identifying machine-generated text. It compares the log probability of a candidate passage under a specific source model with the average log probability of several perturbations of the passage under the same model. This method leverages the observation that machine-generated text tends to lie in regions with negative curvature in the log probability function.\n",
      "\n",
      "2. **AI Tools**: AI tools can be used for text identification, including detecting AI-generated text, analyzing convolutional traces for deepfake detection, measuring how models mimic human falsehoods, authorship attribution in neural text generation, and defending against neural fake news.\n",
      "\n",
      "By utilizing these methods and tools, it is possible to effectively identify machine-generated text and distinguish it from human-authored content.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"How can machine generated text be identified? Use the tools.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066d18e",
   "metadata": {},
   "source": [
    "__Queries that utilizes `multiply_tool` and `add_tool` to perform somewhat complex mathematical calculations for which LLMs normally produces wrong answers.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "360751d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is (121 * 3) + 42?\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\"a\":121,\"b\":3}\n",
      "Got output: 363\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\":363,\"b\":42}\n",
      "Got output: 405\n",
      "========================\n",
      "\n",
      "The result of (121 * 3) + 42 is 405.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is (121 * 3) + 42?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1b0cf0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: What is (44.5 + 7.3) * (9.6 + 5)?\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\":44.5,\"b\":7.3}\n",
      "Got output: 51.8\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\":9.6,\"b\":5}\n",
      "Got output: 14.6\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\"a\":51.8,\"b\":14.6}\n",
      "Got output: 756.28\n",
      "========================\n",
      "\n",
      "The result of (44.5 + 7.3) * (9.6 + 5) is 756.28.\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What is (44.5 + 7.3) * (9.6 + 5)?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb28259",
   "metadata": {},
   "source": [
    "__Complex query for which the agent has to use multiple tools__\n",
    "\n",
    "Agent runs multiple tools by making multiple function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "bb40cf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain transformers. Once you are done, explain how are transformers involved in DetectGPT.  Then, add 2 and 3 then multiply the sum by 4.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Explanation of transformers\"}\n",
      "Got output: Transformers are a network architecture that relies solely on attention mechanisms, eliminating the need for recurrence and convolutions. They are designed to capture global dependencies between input and output sequences without considering their distance. This architecture allows for enhanced parallelization during training, leading to faster training times and improved translation quality.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Role of transformers in DetectGPT\"}\n",
      "Got output: Transformers play a crucial role in DetectGPT by serving as the foundational architecture for sequence transduction models based entirely on attention. They replace the traditional recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention. This transition allows for faster training compared to models based on recurrent or convolutional layers, leading to state-of-the-art performance in translation tasks like English-to-German and English-to-French. The future application of attention-based models, including transformers, is anticipated in various tasks beyond text, such as handling images, audio, and video inputs efficiently.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\":2,\"b\":3}\n",
      "Got output: 5\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\"a\":5,\"b\":4}\n",
      "Got output: 20\n",
      "========================\n",
      "\n",
      "Transformers are a network architecture that relies solely on attention mechanisms, eliminating the need for recurrence and convolutions. They are designed to capture global dependencies between input and output sequences without considering their distance. This architecture allows for enhanced parallelization during training, leading to faster training times and improved translation quality.\n",
      "\n",
      "In the context of DetectGPT, transformers play a crucial role by serving as the foundational architecture for sequence transduction models based entirely on attention. They replace the traditional recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention. This transition allows for faster training compared to models based on recurrent or convolutional layers, leading to state-of-the-art performance in translation tasks like English-to-German and English-to-French. The future application of attention-based models, including transformers, is anticipated in various tasks beyond text, such as handling images, audio, and video inputs efficiently.\n",
      "\n",
      "Adding 2 and 3 gives a sum of 5, and multiplying this sum by 4 results in 20.\n"
     ]
    }
   ],
   "source": [
    "tmp = agent.chat(\"Explain transformers. Once you are done, explain how are transformers involved in DetectGPT. \"\n",
    "    \" Then, add 2 and 3 then multiply the sum by 4.\")\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab4ae5e",
   "metadata": {},
   "source": [
    "__Stream Chat: Streams response instantly__\n",
    "\n",
    "Response is give instantly by sending the answer piece-by-piece as the LLM thinks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0871e14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Explain transformers. Once you are done, explain how are transformers involved in DetectGPT.  Then, add 2 and 3 then multiply the sum by 4.\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Explanation of transformers\"}\n",
      "Got output: Transformers are a network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional layers typically used in sequence transduction models. By leveraging attention to capture global dependencies between input and output sequences, transformers enable more efficient parallelization during training, leading to faster training times and improved performance. This architecture has shown superior quality in tasks like machine translation, achieving state-of-the-art results with significantly reduced training costs compared to traditional models.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: query_engine_tool with args: {\"input\": \"Role of transformers in DetectGPT\"}\n",
      "Got output: Transformers play a crucial role in DetectGPT by serving as the foundational architecture for sequence transduction models based entirely on attention. They replace the traditional recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention. This transition to attention mechanisms allows for faster training compared to models based on recurrent or convolutional layers, making them highly efficient for translation tasks. The Transformer model has demonstrated state-of-the-art performance on various translation tasks, surpassing previously reported ensembles. Additionally, the flexibility of transformers opens up possibilities for extending their application beyond text-related tasks to handle input and output modalities like images, audio, and video efficiently.\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: add with args: {\"a\":2,\"b\":3}\n",
      "Got output: 5\n",
      "========================\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: multiply with args: {\"a\":5,\"b\":4}\n",
      "Got output: 20\n",
      "========================\n",
      "\n",
      "Transformers are a network architecture that relies solely on attention mechanisms, eliminating the need for recurrent or convolutional layers typically used in sequence transduction models. By leveraging attention to capture global dependencies between input and output sequences, transformers enable more efficient parallelization during training, leading to faster training times and improved performance. This architecture has shown superior quality in tasks like machine translation, achieving state-of-the-art results with significantly reduced training costs compared to traditional models.\n",
      "\n",
      "In the context of DetectGPT, transformers play a crucial role by serving as the foundational architecture for sequence transduction models based entirely on attention. They replace the traditional recurrent layers commonly used in encoder-decoder architectures with multi-headed self-attention. This transition to attention mechanisms allows for faster training compared to models based on recurrent or convolutional layers, making them highly efficient for translation tasks. The Transformer model has demonstrated state-of-the-art performance on various translation tasks, surpassing previously reported ensembles. Additionally, the flexibility of transformers opens up possibilities for extending their application beyond text-related tasks to handle input and output modalities like images, audio, and video efficiently.\n",
      "\n",
      "Adding 2 and 3 gives a sum of 5, and multiplying this sum by 4 results in 20."
     ]
    }
   ],
   "source": [
    "response = agent.stream_chat(\n",
    "    \"Explain transformers. Once you are done, explain how are transformers involved in DetectGPT. \"\n",
    "    \" Then, add 2 and 3 then multiply the sum by 4.\"\n",
    ")\n",
    "\n",
    "response_gen = response.response_gen\n",
    "\n",
    "for token in response.response_gen:\n",
    "    print(token, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24893b25",
   "metadata": {},
   "source": [
    "### Storage Context\n",
    "\n",
    "No need to parse data or documents everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "22b9dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe4b49c",
   "metadata": {},
   "source": [
    "__Persist data in a vector database__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5a2c6211",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir=\"storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "56beca45",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(persist_dir=\"storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa568b94",
   "metadata": {},
   "source": [
    "__Load stored data and create query engine__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "406c3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "64acf7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_from_saved = saved_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559d7f45",
   "metadata": {},
   "source": [
    "__Query with stored data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "5d9f805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for DetectGPT is measured using the BLEU score, which is a metric commonly used to evaluate the quality of machine translation.\n"
     ]
    }
   ],
   "source": [
    "res0 = query_engine_from_saved.query(\"How is the accuracy for DetectGPT measured? Explain.\")\n",
    "print(res0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3005ff56",
   "metadata": {},
   "source": [
    "### Chat with data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a2b6c",
   "metadata": {},
   "source": [
    "__Context aware and can ask follow up questions using saved message history.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "24d506bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine.context import ContextChatEngine\n",
    "from llama_index.core.base.llms.types import ChatMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae826c23",
   "metadata": {},
   "source": [
    "__Sample message history from the last query__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b1e833e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "messageHistory = [\n",
    "    ChatMessage(\n",
    "        role = \"user\",\n",
    "        content = \"How is the accuracy for DetectGPT measured? Explain.\"\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role = \"assistant\",\n",
    "        content = \"The accuracy for DetectGPT is measured using the BLEU score, which is a metric commonly used to evaluate the quality of machine translation.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd023870",
   "metadata": {},
   "source": [
    "__Context aware chat engine__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "793b31ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever2 = saved_index.as_retriever()\n",
    "retriever2.similarityTopK = 3\n",
    "\n",
    "chatEngine = ContextChatEngine.from_defaults(retriever=retriever2, chat_history=messageHistory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ed75df",
   "metadata": {},
   "source": [
    "__Query with context aware chat engine about previous response__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "4a7eaf34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I mentioned that the BLEU score is used to assess the accuracy of the English-to-German translation produced by the Transformer model in the context of the paper.\n"
     ]
    }
   ],
   "source": [
    "res9 = chatEngine.chat(\"What was the last thing you mentioned?\")\n",
    "print(res9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0375c6",
   "metadata": {},
   "source": [
    "Can easily recall previous responses allowing for follow-up questions keeping context in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ca30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
